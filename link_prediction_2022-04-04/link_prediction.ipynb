{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68961b3f-0903-4a73-8af6-0b12b3ce363d",
   "metadata": {},
   "source": [
    "### Latex Macros\n",
    "$\\newcommand{\\Re}[1]{{\\mathbb{R}^{{#1}}}}\n",
    "\\newcommand{\\Rez}{{\\mathbb{R}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286ef6ee-594b-4ad7-8250-d595ccf7c5b1",
   "metadata": {},
   "source": [
    "# Basics of link prediction\n",
    "As we discussed last time, recommender systems can be viewed as an exercise in link prediction. \n",
    "From simplest to more complex, the types of graphs that come into  play are: \n",
    "1) Undirected or directed graphs\n",
    "2) Bipartite Graphs\n",
    "3) Heterogeneous graphs\n",
    "    * Heterogeneous nodes and edges\n",
    "4) Temporal graphs\n",
    "    * homogeneous\n",
    "    * heterogeneous\n",
    "    * multi-graphs\n",
    "    * hypergraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ed9561-e4fc-415a-95ef-25f461d1df31",
   "metadata": {},
   "source": [
    "# Link-prediction algorithms\n",
    "Hamilton (2020)](https://www.cs.mcgill.ca/~wlh/grl_book/), considers the following broad approaches\n",
    "1. Node similarity: <br/>\n",
    "    Each node is assigned a similarity value (a number $S\\in\\Rez$). This can be used as a node features. \n",
    "2. A similarity $S(u,v)$ is defined between nodes $u,v$. Rank potential links $(u,v)$ according to $S(u,v)$. <br/>\n",
    "\n",
    "## Examples of similarity metrics: \n",
    "### Neighborhood overlap measures\n",
    "* Sorenson overlap\n",
    "* Salton overlap\n",
    "* Jaccard overlap: $$ \\frac{|\\cal{N}(u) \\cap \\cal{N}(v)|}{|\\cal{N}(u) \\cup \\cal{N}(v)|}$$\n",
    "* Reource Allocation (RA) index: \n",
    "$$  S_{RA}(v_1, v_2) = \\sum_{u\\in\\cal{N}(v_1)\\cap\\cal{N}(v_2)} \\frac{1}{d_u} $$\n",
    "* Adamic-Adar (AA) index: \n",
    "$$  S_{AA}(v_1,v_2) = \\sum_{u\\in\\cal{N}(v_1)\\cap\\cal{N}(v_2)} \\frac{1}{\\log(d_u)} $$\n",
    "\n",
    "### Global overlap measures\n",
    "* Katz index\n",
    "$$ S_{Katz}(u,v) = \\sum_{i=1}^\\infty \\beta_i A^i(u,v) $$ with $\\beta\\in\\Rez$. \n",
    "* Normalized Katz index:\n",
    "$$\n",
    "S_{LNH}(u,v) = I(u,v) + \\frac{2m}{d_u d_v} \\sum_{i=0}^\\infty \\beta^i\\lambda_1^(1-i} A^i(u,v) \n",
    "$$ \n",
    "with the analytical simplification: \n",
    "$$\n",
    " S_{LNH}(u,v) = 2 \\alpha m \\lambda_1 D^{-1} (I - \\frac{\\beta}{\\lambda_1} A)^{-1} D^{-1},\n",
    "$$\n",
    "where\n",
    "$I \\in \\Re{|V|\\times|V|}$ identity matrix, $\\lambda_1$ is maximum eigenvalue of $A$. \n",
    "\n",
    "### Random Walk measures\n",
    "They are reminiscent of the algorithm word2vec. Start from two nodes and execute pairs of random walks from nodes $u$ and $j$ based on the \n",
    "transition matrix $P=AD^{-1}$. Measure the common nodes between the random walks. \n",
    "\n",
    "More deterministic, solve for the steady state of the random walk, defined as $q_u$. <br/>\n",
    "$q_u(v)$ is the probability that a path starting at node $u$ ends at node $v$  Define a random walk similarity measure as\n",
    "$$\n",
    "S_{RW}(u,v) = q_u(v)+ q_v(u)\n",
    "$$\n",
    "\n",
    "There are many more metrics that can and are used. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1467cee6-e7bc-402a-a853-2f441fa2146b",
   "metadata": {},
   "source": [
    "## Using similarity metrics for link preduction\n",
    "Given a graph $G(V,E)$, consider the existence of links on a set of edges $e_{i} \\in \\cal{E}$. <br/>\n",
    "Compute $S(e_i[0], e_i[1])$ for all edges in $\\cal{E}$ and rank the edges. Top $n$ edges are the most likely new links. \n",
    "\n",
    "## Disadvantage of similarity metrics\n",
    "* need hand-engineered statistics\n",
    "* features cannot be learned\n",
    "* the node representations are given\n",
    "\n",
    "## Objectives\n",
    "* learn the node and edge representations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc11b18-f3c6-415a-824b-cf7b2fa6afc2",
   "metadata": {},
   "source": [
    "<img src=\"images/orig_embedding_hamilton.png\" width=\"800\"/>\n",
    "ENC(u) transforms a node to an embedding. Any type of network can be used, but usually some form of GNN. \n",
    "* Shallow encoders only consider graph topology through metrics\n",
    "* GNN can also consider node and edge features\n",
    "* GNN can balance the effect of topology and that of features obtained via social networks for example. \n",
    "\n",
    "## Standard decoder\n",
    "* encode all the does to embeddings $Z = {z(u)}$\n",
    "* decode the embeddings to reconstruct the neighborhood. \n",
    "\n",
    "<img src=\"images/standard_decoder_hamilton.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20f77db-c066-46c9-ba1a-e6089c063c5b",
   "metadata": {},
   "source": [
    "## Pairwise decoders\n",
    "$\n",
    "\\hspace{2in} DEC:  \\Re{d} \\times \\Re{d} ==>  \\Rez^+\n",
    "$\n",
    "\n",
    "Interpretation: output of decoder measures the similarity between two nodes. \n",
    "\n",
    "## Supervised learning\n",
    "Given a collection of edges, compute a similarity measure $S(u,v)$. The objective is \n",
    "for the decoder to *reconstruct* $S(u,v)$.\n",
    "\n",
    "## Loss function\n",
    "$\n",
    "\\hspace{2in}\\cal{L} = \\sum_{(u,v)\\in\\cal{D}} l(\\rm{DEC}(z_u, z_v), S(u,v))\n",
    "$\n",
    "\n",
    "<img src=\"images/similarity_table_hamilton.png\" width=800/>\n",
    "\n",
    "Different choices of decoder, similrity measure and loss function lead to different approaches. No \n",
    "approach is best for all tasks. \n",
    "\n",
    "Please read Hamilton Book, chapters 2 and 3 to review the above material."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f770ccf-7a01-40ce-92a4-8a98099a9d88",
   "metadata": {},
   "source": [
    "# Analyze code written to use GraphSage to solve a link prediction problem\n",
    "Code and notes are taken from the blog of [ Samar Khanna, Sarthak Consul, and Tanish Jain, Stanford students](https://medium.com/stanford-cs224w/online-link-prediction-with-graph-neural-networks-46c1054f2aa4), which describes how to use GraphSage for link prediction.\n",
    "\n",
    "## Colab notebook\n",
    "* https://colab.research.google.com/drive/1nb-BhjggQq4E7Ew9RxlydUbXF7Tpk6mH\n",
    "\n",
    "## Github repository\n",
    "* https://github.com/samar-khanna/cs224w-project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7345c04a-d6db-4b11-8e23-cfb8c639ad0f",
   "metadata": {},
   "source": [
    "We will analyze the structure of the code, and run it next week. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a856c8e-3080-4324-a2f2-2100d32ffbc6",
   "metadata": {},
   "source": [
    "* The code uses [pyTorch_geometric](https://pytorch-geometric.readthedocs.io/en/latest/), and OGB ([Open Graph Benchmark](https://ogb.stanford.edu))\n",
    "It is important to analyze source code and learn as you code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7276ce03-94d6-4528-b13d-ff9fd41db78a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch_geometric'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a4f5dddbdb93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_geometric\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpyg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnegative_sampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch_geometric'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric as pyg\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from ogb.linkproppred import PygLinkPropPredDataset, Evaluator #needed to extract and evaluate the ogb-ddi dataset\n",
    "import matplotlib.pyplot as plt #needed to visualize loss curves\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8a985f-637d-4f07-a903-85c6e3894869",
   "metadata": {},
   "source": [
    "## GraphSage model definition\n",
    "Found in the [PyGeometric Docs](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html?highlight=nn.sageconv#torch_geometric.nn.conv.SAGEConv). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bd77e3b-0621-4245-b24e-1d9e90cc6f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNStack(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, dropout, emb=False):\n",
    "        super(GNNStack, self).__init__()\n",
    "        \n",
    "        # Model obtained from pyg\n",
    "        # nn.SAGEConv probably becomes nn.conv.SAGEConv\n",
    "        conv_model = pyg.nn.SAGEConv\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(conv_model(input_dim, hidden_dim))\n",
    "        self.dropout = dropout\n",
    "        self.num_layers = num_layers\n",
    "        self.emb = emb\n",
    "\n",
    "        # Create num_layers GraphSAGE convs\n",
    "        assert (self.num_layers >= 1), 'Number of layers is not >=1'\n",
    "        for l in range(self.num_layers - 1):\n",
    "            self.convs.append(conv_model(hidden_dim, hidden_dim))\n",
    "\n",
    "        # post-message-passing processing \n",
    "        self.post_mp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.Dropout(self.dropout),\n",
    "            nn.Linear(hidden_dim, output_dim))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # x is the initial embedding\n",
    "        # In this code, it is the weight from an embedding layer. \n",
    "        # If there were node attributes, x would be the initial node attributes\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        x = self.post_mp(x)\n",
    "\n",
    "        # Return final layer of embeddings if specified\n",
    "        if self.emb:\n",
    "            return x\n",
    "\n",
    "        # Else return class probabilities\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def loss(self, pred, label):\n",
    "        return F.nll_loss(pred, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155526d5-ee1d-4ac5-9bcc-fdc49ce4376a",
   "metadata": {},
   "source": [
    "* This model has multiple layers, an ReLU activation function, uses dropout\n",
    "* The forward function returns either embeddings, or a softmax (if a probability is required)\n",
    "* Input to the forward function takes x (list of nodes) and edge_index: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fff753d-0c22-44f6-a79e-f42892b59d4c",
   "metadata": {},
   "source": [
    "## Decoder definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "581dbc70-a11f-4b38-a21a-b4a8c5ad5c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkPredictor(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout):\n",
    "        super(LinkPredictor, self).__init__()\n",
    "\n",
    "        # Create linear layers\n",
    "        self.lins = nn.ModuleList()\n",
    "        self.lins.append(nn.Linear(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.lins.append(nn.Linear(hidden_channels, hidden_channels))\n",
    "        self.lins.append(nn.Linear(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for lin in self.lins:\n",
    "            lin.reset_parameters()\n",
    "\n",
    "    def forward(self, x_i, x_j):\n",
    "        # x_i and x_j are both of shape (E, D)\n",
    "        x = x_i * x_j\n",
    "        for lin in self.lins[:-1]:\n",
    "            x = lin(x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.lins[-1](x)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaec54c6-1f42-4018-8922-303d08cc63cc",
   "metadata": {},
   "source": [
    "* Input to the *forward* method is two nodes. \n",
    "* x is the pointwise multiplication between the embeddings of nodes i and j. \n",
    "* The link predictor returns a probablity. \n",
    "    * assume a link if probability > 0.5\n",
    "    * no link if probability < 0.5 ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204a73be-22c9-4b57-a84e-cfee0b26aa5a",
   "metadata": {},
   "source": [
    "## Training\n",
    "Source code to [negative_sampling](https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/utils/negative_sampling.html) in PyGeometric.<br/> \n",
    "The *negative_sampling* method only considers true negative samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b203009-d230-466f-8466-899e43f1a717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, link_predictor, emb, edge_index, pos_train_edge, batch_size, optimizer):\n",
    "    \"\"\"\n",
    "    Runs offline training for model, link_predictor and node embeddings given the message\n",
    "    edges and supervision edges.\n",
    "    :param model: Torch Graph model used for updating node embeddings based on message passing\n",
    "    :param link_predictor: Torch model used for predicting whether edge exists or not\n",
    "    :param emb: (N, d) Initial node embeddings for all N nodes in graph\n",
    "    :param edge_index: (2, E) Edge index for all edges in the graph\n",
    "    :param pos_train_edge: (PE, 2) Positive edges used for training supervision loss\n",
    "    :param batch_size: Number of positive (and negative) supervision edges to sample per batch\n",
    "    :param optimizer: Torch Optimizer to update model parameters\n",
    "    :return: Average supervision loss over all positive (and correspondingly sampled negative) edges\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    link_predictor.train()\n",
    "\n",
    "    train_losses = []\n",
    "\n",
    "    for edge_id in DataLoader(range(pos_train_edge.shape[0]), batch_size, shuffle=True):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Run message passing on the inital node embeddings to get updated embeddings\n",
    "        # emb: initial node embedding <<<<<\n",
    "        # edge_index: list of edges (2,E)\n",
    "        node_emb = model(emb, edge_index)  # (N, d)\n",
    "\n",
    "        # Predict the class probabilities on the batch of positive edges using link_predictor\n",
    "        pos_edge = pos_train_edge[edge_id].T  # (2, B)\n",
    "        pos_pred = link_predictor(node_emb[pos_edge[0]], node_emb[pos_edge[1]])  # (B, )\n",
    "\n",
    "        # Sample negative edges (same number as number of positive edges) and predict class probabilities \n",
    "        # negative_sampling is built into torch_geometric\n",
    "        neg_edge = negative_sampling(edge_index, num_nodes=emb.shape[0],\n",
    "                                     num_neg_samples=edge_id.shape[0], method='dense')  # (Ne,2)\n",
    "        neg_pred = link_predictor(node_emb[neg_edge[0]], node_emb[neg_edge[1]])  # (Ne,)\n",
    "\n",
    "        # Compute the corresponding negative log likelihood loss on the positive and negative edges\n",
    "        loss = -torch.log(pos_pred + 1e-15).mean() - torch.log(1 - neg_pred + 1e-15).mean()\n",
    "\n",
    "        # Backpropagate and update parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "    return sum(train_losses) / len(train_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b873cd-f10f-48b0-b22f-06ad97237fdf",
   "metadata": {},
   "source": [
    "## Testing\n",
    "Use the Hits@K metric ([more info](https://stackoverflow.com/questions/58796367/how-is-hitsk-calculated-and-what-does-it-mean-in-the-context-of-link-prediction))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c0c68bd-c286-45a4-83a4-183f702c7ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, predictor, emb, edge_index, split_edge, batch_size, evaluator):\n",
    "    \"\"\"\n",
    "    Evaluates graph model on validation and test edges\n",
    "    :param model: Torch Graph model used for updating node embeddings based on message passing\n",
    "    :param predictor: Torch model used for predicting whether edge exists or not\n",
    "    :param emb: (N, d) Initial node embeddings for all N nodes in graph\n",
    "    :param edge_index: (2, E) Edge index for all edges in the graph\n",
    "    :param split_edge: Dictionary of (e, 2) edges for val pos/neg and test pos/neg edges\n",
    "    :param batch_size: Number of positive (and negative) supervision edges to sample per batch\n",
    "    :param evaluator: OGB evaluator to calculate hits @ k metric\n",
    "    :return: hits @ k results\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictor.eval()\n",
    "\n",
    "    node_emb = model(emb, edge_index)\n",
    "\n",
    "    pos_valid_edge = split_edge['valid']['edge'].to(emb.device)\n",
    "    neg_valid_edge = split_edge['valid']['edge_neg'].to(emb.device)\n",
    "    pos_test_edge = split_edge['test']['edge'].to(emb.device)\n",
    "    neg_test_edge = split_edge['test']['edge_neg'].to(emb.device)\n",
    "\n",
    "    pos_valid_preds = []\n",
    "    for perm in DataLoader(range(pos_valid_edge.size(0)), batch_size):\n",
    "        edge = pos_valid_edge[perm].t()\n",
    "        pos_valid_preds += [predictor(node_emb[edge[0]], node_emb[edge[1]]).squeeze().cpu()]\n",
    "    pos_valid_pred = torch.cat(pos_valid_preds, dim=0)\n",
    "\n",
    "    neg_valid_preds = []\n",
    "    for perm in DataLoader(range(neg_valid_edge.size(0)), batch_size):\n",
    "        edge = neg_valid_edge[perm].t()\n",
    "        neg_valid_preds += [predictor(node_emb[edge[0]], node_emb[edge[1]]).squeeze().cpu()]\n",
    "    neg_valid_pred = torch.cat(neg_valid_preds, dim=0)\n",
    "\n",
    "    pos_test_preds = []\n",
    "    for perm in DataLoader(range(pos_test_edge.size(0)), batch_size):\n",
    "        edge = pos_test_edge[perm].t()\n",
    "        pos_test_preds += [predictor(node_emb[edge[0]], node_emb[edge[1]]).squeeze().cpu()]\n",
    "    pos_test_pred = torch.cat(pos_test_preds, dim=0)\n",
    "\n",
    "    neg_test_preds = []\n",
    "    for perm in DataLoader(range(neg_test_edge.size(0)), batch_size):\n",
    "        edge = neg_test_edge[perm].t()\n",
    "        neg_test_preds += [predictor(node_emb[edge[0]], node_emb[edge[1]]).squeeze().cpu()]\n",
    "    neg_test_pred = torch.cat(neg_test_preds, dim=0)\n",
    "\n",
    "    results = {}\n",
    "    for K in [20, 50, 100]:\n",
    "        evaluator.K = K\n",
    "        valid_hits = evaluator.eval({\n",
    "            'y_pred_pos': pos_valid_pred,\n",
    "            'y_pred_neg': neg_valid_pred,\n",
    "        })[f'hits@{K}']\n",
    "        test_hits = evaluator.eval({\n",
    "            'y_pred_pos': pos_test_pred,\n",
    "            'y_pred_neg': neg_test_pred,\n",
    "        })[f'hits@{K}']\n",
    "\n",
    "        results[f'Hits@{K}'] = (valid_hits, test_hits)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8a38c5-f394-4238-8e87-512d1e7be302",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "* The [link prediction dataset](https://ogb.stanford.edu/docs/linkprop/) can be downloaded directly from within OGB. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8836818-6f31-4ce3-9438-04f2c4216e42",
   "metadata": {},
   "source": [
    "dataset = PygLinkPropPredDataset(name=\"ogbl-ddi\", root='./dataset/') #download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e0aa7d6-28ca-497e-9d2f-c16d3a86ad8d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Hyperparameters\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "optim_wd = 0\n",
    "epochs = 300\n",
    "hidden_dim = 256\n",
    "dropout = 0.3\n",
    "num_layers = 2\n",
    "lr = 3e-3\n",
    "node_emb_dim = 256\n",
    "batch_size = 64 * 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4a65b3-edbb-48a8-b308-ebb5264a3f30",
   "metadata": {},
   "source": [
    "## Run the code\n",
    "* Note that initial node embeddings are specified via an Embeddings layer, i.e., a weight matrix. \n",
    "* More genreally, the initial embeddings would be the node features (none in this case). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f3dedce-10d6-4ffc-85fb-f63c0339c85e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-5944c6522190>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msplit_edge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_edge_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpos_train_edge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_edge\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'edge'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0medge_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "split_edge = dataset.get_edge_split()\n",
    "pos_train_edge = split_edge['train']['edge'].to(device)\n",
    "\n",
    "graph = dataset[0]\n",
    "edge_index = graph.edge_index.to(device)\n",
    "\n",
    "evaluator = Evaluator(name='ogbl-ddi')\n",
    "\n",
    "# The nodes have no features. The initial embeddings are random numbers. \n",
    "#  GE: More generally, the initial embeddings would be node features. \n",
    "emb = torch.nn.Embedding(graph.num_nodes, node_emb_dim).to(device) # each node has an embedding that has to be learnt\n",
    "\n",
    "# Define the Encoder\n",
    "model = GNNStack(node_emb_dim, hidden_dim, hidden_dim, num_layers, dropout, emb=True).to(device) # the graph neural network that takes all the node embeddings as inputs to message pass and agregate\n",
    "\n",
    "# Define the Decoder\n",
    "link_predictor = LinkPredictor(hidden_dim, hidden_dim, 1, num_layers + 1, dropout).to(device) # the MLP that takes embeddings of a pair of nodes and predicts the existence of an edge between them\n",
    "\n",
    "# Note the parameters: model, Link, embedding matrix\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(model.parameters()) + list(link_predictor.parameters()) + list(emb.parameters()),\n",
    "    lr=lr, weight_decay=optim_wd\n",
    ")\n",
    "\n",
    "train_loss = []\n",
    "val_hits = []\n",
    "test_hits = []\n",
    "for e in range(epochs):\n",
    "    # GE: notice the input emb.weight. How is it used? \n",
    "    loss = train(model, link_predictor, emb.weight, edge_index, pos_train_edge, batch_size, optimizer)\n",
    "    print(f\"Epoch {e + 1}: loss: {round(loss, 5)}\")\n",
    "    train_loss.append(loss)\n",
    "\n",
    "    if (e+1)%10 ==0:\n",
    "        result = test(model, link_predictor, emb.weight, edge_index, split_edge, batch_size, evaluator)\n",
    "        val_hits.append(result['Hits@20'][0])\n",
    "        test_hits.append(result['Hits@20'][1])\n",
    "        print(result)\n",
    "\n",
    "plt.title('Link Prediction on OGB-ddi using GraphSAGE GNN')\n",
    "plt.plot(train_loss,label=\"training loss\")\n",
    "plt.plot(np.arange(9,epochs,10),val_hits,label=\"Hits@20 on validation\")\n",
    "plt.plot(np.arange(9,epochs,10),test_hits,label=\"Hits@20 on test\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162ed533-d9b1-44b3-98e6-ae06630a915b",
   "metadata": {},
   "source": [
    "# Online Link Predictor\n",
    "* For next week. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcc2345-afe6-4842-acb9-66281571eb15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": true,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
